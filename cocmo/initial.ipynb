{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10060] A connection attempt failed because\n",
      "[nltk_data]     the connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "import operator\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import elementtree_indent\n",
    "import operator\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "with open(\"captions/annotations/captions_val2014.json\") as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "anno_df = pd.DataFrame(data['annotations'])\n",
    "image_df = pd.DataFrame(data['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anno_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-66fe9b7c0a5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manno_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'anno_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(anno_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inverted_index:\n",
    "    def __init__(self):\n",
    "        url = 'captions/annotations/captions_val2014.json'\n",
    "        anno_df, image_df = self.get_dataset(url)\n",
    "        anno_df['caption'] = anno_df['caption'].apply(self.text_preprocess)\n",
    "        self.index = self.generate_inverted_index(anno_df[\"caption\"].head(10))\n",
    "        self.index = self.to_df(self.index)\n",
    "\n",
    "    def get_dataset(self, path):\n",
    "        with open(\"captions/annotations/captions_val2014.json\") as json_data:\n",
    "          data = json.load(json_data)\n",
    "        anno_df = pd.DataFrame(data['annotations'])\n",
    "        image_df = pd.DataFrame(data['images'])\n",
    "        return anno_df, image_df\n",
    "\n",
    "    def text_preprocess(self, text):\n",
    "        nltk_english_stopwords = stopwords.words('english')\n",
    "        trans = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(trans)\n",
    "        text = text.lower()\n",
    "        cleaned_text = \"\"\n",
    "        for word in text.split():\n",
    "            if word not in nltk_english_stopwords:\n",
    "                cleaned_text += word + \" \" \n",
    "        return cleaned_text \n",
    "    \n",
    "    def generate_inverted_index(self, data: list):\n",
    "        inv_idx_dict = {}\n",
    "        for index, doc_text in enumerate(data):\n",
    "            words = word_tokenize(doc_text)\n",
    "            for word in words:\n",
    "                if  ps.stem(word) not in inv_idx_dict.keys():\n",
    "                    inv_idx_dict[ ps.stem(word)] = [index]\n",
    "                elif index not in inv_idx_dict[ ps.stem(word)]:\n",
    "                    inv_idx_dict[ ps.stem(word)].append(index)\n",
    "        return inv_idx_dict\n",
    "    \n",
    "    def to_df(self, dict):\n",
    "        df = pd.DataFrame();\n",
    "        v = []\n",
    "        i = 0\n",
    "        for key, values in dict.items():\n",
    "            df.loc[i,'key'] = key\n",
    "            strval = ''\n",
    "            for value in values:\n",
    "                strval += str(value) + ' '\n",
    "            df.loc[i,'values'] = strval\n",
    "            i = i +1\n",
    "        return df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          key    values\n",
      "0      bicycl        0 \n",
      "1     replica        0 \n",
      "2       clock      0 9 \n",
      "3       front      0 1 \n",
      "4       wheel        0 \n",
      "5       black        1 \n",
      "6       honda        1 \n",
      "7   motorcycl        1 \n",
      "8        park      1 3 \n",
      "9       garag        1 \n",
      "10       room        2 \n",
      "11       blue  2 6 7 8 \n",
      "12       wall    2 7 8 \n",
      "13      white    2 6 7 \n",
      "14       sink      2 7 \n",
      "15       door        2 \n",
      "16        car        3 \n",
      "17       seem        3 \n",
      "18      illeg        3 \n",
      "19     behind        3 \n",
      "20      legal        3 \n",
      "21       larg        4 \n",
      "22    passeng        4 \n",
      "23    airplan        4 \n",
      "24        fli        4 \n",
      "25        air        4 \n",
      "26        gol        5 \n",
      "27      plane        5 \n",
      "28       take        5 \n",
      "29     partli        5 \n",
      "30     cloudi        5 \n",
      "31        sky        5 \n",
      "32      color        6 \n",
      "33     scheme        6 \n",
      "34      small        6 \n",
      "35   bathroom    6 7 8 \n",
      "36    lifesav        7 \n",
      "37       boat        8 \n",
      "38      theme        8 \n",
      "39       life        8 \n",
      "40    preserv        8 \n",
      "41       bike        9 \n",
      "42       tire        9 \n"
     ]
    }
   ],
   "source": [
    "inv_index = inverted_index()\n",
    "index = inv_index.index\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class engine:\n",
    "    def __init__(self, index: pd.DataFrame):\n",
    "        self.vocabulary = list(index['key'])\n",
    "        self.tfidf = TfidfVectorizer(vocabulary=self.vocabulary)\n",
    "        self.tfidf.fit(index['key'])\n",
    "        self.tfidf_tran = self.tfidf.transform(index['key'])\n",
    "        \n",
    "    \n",
    "    def gen_vector_T(self, tokens):\n",
    "        Q = np.zeros((len(self.vocabulary)))    \n",
    "        x= self.tfidf.transform(tokens)\n",
    "        for token in tokens[0].split(','):\n",
    "            try:\n",
    "                ind = self.vocabulary.index(token)\n",
    "                Q[ind]  = x[0, self.tfidf.vocabulary_[token]]\n",
    "            except:\n",
    "                pass\n",
    "        return Q\n",
    "\n",
    "    def cosine_sim(self,a, b):\n",
    "        cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "        return cos_sim\n",
    "\n",
    "    def search(self, query, index):\n",
    "        tokens = word_tokenize(str(query))\n",
    "        clean = []\n",
    "        clean.append(tokens)\n",
    "        query_vector = self.gen_vector_T(tokens)\n",
    "        d_cosines = []\n",
    "        for d in self.tfidf_tran.A:\n",
    "            d_cosines.append(self.cosine_sim(query_vector, d))\n",
    "        out = np.array(d_cosines).argsort()[:][::-1]\n",
    "        a = pd.DataFrame()\n",
    "        for i,ind in enumerate(out):\n",
    "            a.loc[i,'index'] = str(ind)\n",
    "            a.loc[i,'val'] = list(index['values'])[ind]\n",
    "        return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index       val\n",
      "0     13    2 6 7 \n",
      "1     42        9 \n",
      "2     10        2 \n",
      "3     18        3 \n",
      "4     17        3 \n",
      "5     16        3 \n",
      "6     15        2 \n",
      "7     14      2 7 \n",
      "8     12    2 7 8 \n",
      "9     11  2 6 7 8 \n",
      "10     9        1 \n",
      "11    20        3 \n",
      "12     8      1 3 \n",
      "13     7        1 \n",
      "14     6        1 \n",
      "15     5        1 \n",
      "16     4        0 \n",
      "17     3      0 1 \n",
      "18     2      0 9 \n",
      "19     1        0 \n",
      "20    19        3 \n",
      "21    21        4 \n",
      "22    41        9 \n",
      "23    32        6 \n",
      "24    40        8 \n",
      "25    39        8 \n",
      "26    38        8 \n",
      "27    37        8 \n",
      "28    36        7 \n",
      "29    35    6 7 8 \n",
      "30    34        6 \n",
      "31    33        6 \n",
      "32    31        5 \n",
      "33    22        4 \n",
      "34    30        5 \n",
      "35    29        5 \n",
      "36    28        5 \n",
      "37    27        5 \n",
      "38    26        5 \n",
      "39    25        4 \n",
      "40    24        4 \n",
      "41    23        4 \n",
      "42     0        0 \n"
     ]
    }
   ],
   "source": [
    "e = engine(index)\n",
    "res = e.search('white ', index)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-win_amd64.whl (35.4 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\mohammad dareer\\anaconda3\\lib\\site-packages (from opencv-python) (1.20.1)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.5.64\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_retrive:\n",
    "    def __init__(self, res, df):\n",
    "        self.res = res\n",
    "        self.ind = self.get_index(res)\n",
    "        self.paths = self.get_images(self.ind, df)\n",
    "\n",
    "    def get_index(self, res):\n",
    "        indices = res['val'][0].split()\n",
    "        indint = []\n",
    "        for i in indices:\n",
    "            indint.append(int(i))\n",
    "        print(indint)\n",
    "        return indint\n",
    "    \n",
    "    def get_images(self, indices, df):\n",
    "        img_ids = []\n",
    "        paths = []\n",
    "        #print(df['image_id'][int(indices[0])])\n",
    "        for i in indices:\n",
    "            \n",
    "            img_ids.append(df['image_id'][i])\n",
    "        for i in img_ids:\n",
    "            print(i)\n",
    "            paths.append('val2014\\\\val2014\\COCO_val2014_'+ '0'*(12 - len(str(i))) + str(i) +'.jpg')\n",
    "        return paths\n",
    "\n",
    "    def display_images(self):\n",
    "        for path in self.paths:\n",
    "            print(path)\n",
    "            img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "            cv2.imshow(';;',img)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 7]\n",
      "322141\n",
      "322141\n",
      "322141\n",
      "val2014\\val2014\\COCO_val2014_000000322141.jpg\n",
      "val2014\\val2014\\COCO_val2014_000000322141.jpg\n",
      "val2014\\val2014\\COCO_val2014_000000322141.jpg\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ir = image_retrive(res, anno_df)\n",
    "ir.display_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44ee107329cf4a2b10dbf9dc2a835ad7dfaf7406a03f5d729fde60d5b9868961"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
