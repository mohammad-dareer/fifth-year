{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohammad\n",
      "[nltk_data]     Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Mohammad\n",
      "[nltk_data]     Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mohammad\n",
      "[nltk_data]     Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mohammad Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Mohammad\n",
      "[nltk_data]     Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "war\n",
      "the casuse\n",
      "what\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "text = 'what is the casuse of war'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases[:10]:\n",
    "    print(phrase.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " war casus\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "r = Rake()\n",
    "my_text = \"\"\"\n",
    "what is the casus of war\n",
    "\"\"\"\n",
    "r.extract_keywords_from_text(my_text)\n",
    "keywordList           = []\n",
    "rankedList            = r.get_ranked_phrases_with_scores()\n",
    "keywords =''\n",
    "for keyword in rankedList:\n",
    "  keyword_updated       = keyword[1].split()\n",
    "  keyword_updated_string    = \" \".join(keyword_updated[:2])\n",
    "  keywordList.append(keyword_updated_string)\n",
    "  keywords = keywords + ' ' + keyword_updated_string\n",
    "  if(len(keywordList)>9):\n",
    "    break\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "def pos_tagger(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "\n",
    "def stopword_treatment(tokens):\n",
    "    stopword = stopwords.words('english')\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token[0].lower() not in stopword:\n",
    "            result.append(tuple([token[0].lower(), token[1]]))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag_map = {\n",
    "    'NN': [ wn.NOUN ],\n",
    "    'JJ': [ wn.ADJ, wn.ADJ_SAT ],\n",
    "    'RB': [ wn.ADV ],\n",
    "    'VB': [ wn.VERB ]\n",
    "}\n",
    "\n",
    "def pos_tag_converter(nltk_pos_tag):\n",
    "    root_tag = nltk_pos_tag[0:2]\n",
    "    try:\n",
    "        pos_tag_map[root_tag]\n",
    "        return pos_tag_map[root_tag]\n",
    "    except KeyError:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synsets(tokens):\n",
    "    synsets = []\n",
    "    for token in tokens:\n",
    "        wn_pos_tag = pos_tag_converter(token[1])\n",
    "        if wn_pos_tag == '':\n",
    "            continue\n",
    "        else:\n",
    "            synsets.append(wn.synsets(token[0], wn_pos_tag))\n",
    "    return synsets\n",
    "\n",
    "def get_tokens_from_synsets(synsets):\n",
    "    tokens = {}\n",
    "    for synset in synsets:\n",
    "        for s in synset:\n",
    "            if s.name() in tokens:\n",
    "                tokens[s.name().split('.')[0]] += 1\n",
    "            else:\n",
    "                tokens[s.name().split('.')[0]] = 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernyms(synsets):\n",
    "    hypernyms = []\n",
    "    for synset in synsets:\n",
    "        for s in synset:\n",
    "            hypernyms.append(s.hypernyms())\n",
    "            \n",
    "    return hypernyms\n",
    "\n",
    "def get_tokens_from_hypernyms(synsets):\n",
    "    tokens = {}\n",
    "    for synset in synsets:\n",
    "        for s in synsets:\n",
    "            for ss in s:\n",
    "                if ss.name().split('.')[0] in tokens:\n",
    "                    tokens[(ss.name().split('.')[0])] += 1\n",
    "                else:\n",
    "                    tokens[(ss.name().split('.')[0])] = 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyponyms(synsets):\n",
    "    hyponyms = []\n",
    "    for synset in synsets:\n",
    "        for s in synset:\n",
    "            hyponyms.append(s.hyponyms())\n",
    "            \n",
    "    return hyponyms\n",
    "\n",
    "def get_tokens_from_hyponyms(synsets):\n",
    "    tokens = {}\n",
    "    for synset in synsets:\n",
    "        for s in synsets:\n",
    "            for ss in s:\n",
    "                if ss.name().split('.')[0] in tokens:\n",
    "                    tokens[(ss.name().split('.')[0])] += 1\n",
    "                else:\n",
    "                    tokens[(ss.name().split('.')[0])] = 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def underscore_replacer(tokens):\n",
    "    new_tokens = {}\n",
    "    for key in tokens.keys():\n",
    "        mod_key = re.sub(r'_', ' ', key)\n",
    "        new_tokens[mod_key] = tokens[key]\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(sentence):\n",
    "    tokens = tokenizer(sentence)\n",
    "    tokens = pos_tagger(tokens)\n",
    "    tokens = stopword_treatment(tokens)\n",
    "\n",
    "    synsets = get_synsets(tokens)\n",
    "    synonyms = get_tokens_from_synsets(synsets)\n",
    "    synonyms = underscore_replacer(synonyms)\n",
    "\n",
    "    hypernyms = get_hypernyms(synsets)\n",
    "    hypernyms = get_tokens_from_hypernyms(hypernyms)\n",
    "    hypernyms = underscore_replacer(hypernyms)\n",
    "\n",
    "    hyponyms = get_hyponyms(synsets)\n",
    "    hyponyms = get_tokens_from_hyponyms(hyponyms)\n",
    "    hyponyms = underscore_replacer(hyponyms)\n",
    "\n",
    "    tokens = {**synonyms, **hypernyms}\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cause': 1,\n",
       " 'campaign': 9,\n",
       " 'causal agent': 1,\n",
       " 'lawsuit': 1,\n",
       " 'war': 1,\n",
       " 'origin': 9,\n",
       " 'justification': 9,\n",
       " 'venture': 9,\n",
       " 'physical entity': 9,\n",
       " 'proceeding': 9,\n",
       " 'military action': 9,\n",
       " 'hostility': 9,\n",
       " 'conflict': 9}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_tokens('what is the causes of war ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44ee107329cf4a2b10dbf9dc2a835ad7dfaf7406a03f5d729fde60d5b9868961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
