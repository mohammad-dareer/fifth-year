{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from elasticsearch import Elasticsearch\n",
    "import sklearn.metrics as sl\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import helpers\n",
    "client = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "     basic_auth=(\"elastic\", \"elastic\")\n",
    "\n",
    ")\n",
    "resp = client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true:list, predect:list):\n",
    "    f = 0\n",
    "    for t in true :\n",
    "        for p in predect:\n",
    "            if p == t:\n",
    "                f = f + 1\n",
    "    return (f / len(predect))\n",
    "def recall(true:list, predect:list):\n",
    "    f = 0\n",
    "    for t in true :\n",
    "        for p in predect:\n",
    "            if p == t:\n",
    "                f = f + 1\n",
    "    return f / len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDate():\n",
    "    temp_id = 0\n",
    "    temp_doc_id = []\n",
    "    rel = {}\n",
    "    with open('data\\CISI.REL') as f:\n",
    "        lines = \"\"\n",
    "        for l in f.readlines():\n",
    "            qid = l.strip().split()[0]\n",
    "            if(qid == temp_id):\n",
    "                temp_doc_id.append(l.strip().split()[1])\n",
    "            else:\n",
    "                rel[temp_id] = temp_doc_id\n",
    "                temp_id = qid\n",
    "                temp_doc_id = []\n",
    "                temp_doc_id.append(l.strip().split()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "class QueryProcessor:\n",
    "\n",
    "    def __init__(self, nlp, keep=None):\n",
    "        self.nlp = nlp\n",
    "        self.keep = keep or {'PROPN', 'NUM', 'VERB', 'NOUN', 'ADJ'}\n",
    "\n",
    "    def generate_query(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        query = ' '.join(token.text for token in doc if token.pos_ in self.keep)\n",
    "        return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class noprocess:\n",
    "    def __init__(self, keep=None):\n",
    "        self.truth =[]\n",
    "        self.predect =[]\n",
    "        self.rel = self.getRelData()\n",
    "        self.qry = self.getQryData()\n",
    "        self.keep = keep or {'PROPN', 'NUM', 'VERB', 'NOUN', 'ADJ'}\n",
    "\n",
    "\n",
    "    def getRelData(self):\n",
    "        temp_id = 0\n",
    "        temp_doc_id = []\n",
    "        rel = {}\n",
    "        with open('data\\CISI.REL') as f:\n",
    "            lines = \"\"\n",
    "            for l in f.readlines():\n",
    "                qid = l.strip().split()[0]\n",
    "                if(qid == temp_id):\n",
    "                    temp_doc_id.append(l.strip().split()[1])\n",
    "                else:\n",
    "                    rel[temp_id] = temp_doc_id\n",
    "                    temp_id = qid\n",
    "                    temp_doc_id = []\n",
    "                    temp_doc_id.append(l.strip().split()[1])\n",
    "        return rel\n",
    "\n",
    "    def getQryData(self):\n",
    "        qry_list =[]\n",
    "        qry_id = \"\"\n",
    "        qry_text = \"\"\n",
    "        with open('data\\CISI.QRY') as f:\n",
    "            qry = \"\"\n",
    "            for l in f.readlines():\n",
    "                qry += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "            qry = qry.lstrip(\"\\n\").split(\"\\n\")\n",
    "        for l in qry:\n",
    "            if l.startswith(\".I\"):\n",
    "                qry_list.append({'id':qry_id, 'doc':qry_text.lstrip(\" \")})\n",
    "                qry_id = l.split(\" \")[1].strip()\n",
    "                qry_text = \"\"\n",
    "            \n",
    "            else:\n",
    "                qry_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.\n",
    "        qry_list.remove({'id':'' ,'doc':''})\n",
    "        return qry_list\n",
    "\n",
    "\n",
    "    def removeWh(self ,q):\n",
    "        query = ''\n",
    "        tokens =  word_tokenize(q)\n",
    "        for word, tag in nltk.pos_tag(tokens):\n",
    "            print(word,tag)\n",
    "            if tag in  self.keep:\n",
    "                query = ' '.join(word)\n",
    "        print(\"qqq\",query)\n",
    "        return query\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def searchNoWhword(self,count, index):\n",
    "        p = 0\n",
    "        pi = 0\n",
    "        r = 0\n",
    "        ri = 0\n",
    "        f1 = 0\n",
    "        truth =[]\n",
    "        predect = []\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        for i in range(1,35):\n",
    "            q = QueryProcessor(nlp).generate_query(self.qry[i]['doc'])\n",
    "            query = {\n",
    "                        \"match\": {\n",
    "                            \"doc\": q\n",
    "                        }\n",
    "                    }\n",
    "            ids =[]\n",
    "            docs =[]\n",
    "            resp = client.search(index=index, query=query, size=count)\n",
    "            for hit in resp['hits']['hits']:\n",
    "                ids.append(hit['_source']['id'])\n",
    "                docs.append(hit['_source']['doc'])\n",
    "            predect.append(ids)\n",
    "            truth.append(self.rel[str(i+1)])\n",
    "            pi = precision(self.rel[str(i+1)], ids)\n",
    "            p = p + pi\n",
    "            ri = recall(self.rel[str(i+1)], ids)\n",
    "            r = r + ri\n",
    "            f1 = f1 + (2 * ((pi * ri)/(pi + ri)))\n",
    "        return p / len(predect), r/len(predect), f1/len(predect)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def searchNoSW(self,count,index):\n",
    "        p = 0\n",
    "        pi = 0\n",
    "        r = 0\n",
    "        ri = 0\n",
    "        f1 = 0\n",
    "        truth =[]\n",
    "        predect = []\n",
    "        for i in range(1,35):\n",
    "            q = remove_stopwords(self.qry[i]['doc'] )\n",
    "            query = {\n",
    "                        \"match\": {\n",
    "                            \"doc\": q\n",
    "                        }\n",
    "                    }\n",
    "            ids =[]\n",
    "            docs =[]\n",
    "            resp = client.search(index=index, query=query, size=count)\n",
    "            for hit in resp['hits']['hits']:\n",
    "                ids.append(hit['_source']['id'])\n",
    "                docs.append(hit['_source']['doc'])\n",
    "            predect.append(ids)\n",
    "            truth.append(self.rel[str(i+1)])\n",
    "            pi = precision(self.rel[str(i+1)], ids)\n",
    "            p = p + pi\n",
    "            ri = recall(self.rel[str(i+1)], ids)\n",
    "            r = r + ri\n",
    "            f1 = f1 + (2 * ((pi * ri)/(pi + ri)))\n",
    "        return p / len(predect), r/len(predect), f1/len(predect)\n",
    "    \n",
    "\n",
    "    def searchPR(self,count,index):\n",
    "        p = 0\n",
    "        pi = 0\n",
    "        r = 0\n",
    "        ri = 0\n",
    "        f1 = 0\n",
    "        truth =[]\n",
    "        predect = []\n",
    "        for i in range(1,35):\n",
    "            query = {\n",
    "                        \"match\": {\n",
    "                            \"doc\": self.qry[i]['doc'] \n",
    "                        }\n",
    "                    }\n",
    "            ids =[]\n",
    "            docs =[]\n",
    "            resp = client.search(index=index, query=query, size=count)\n",
    "            for hit in resp['hits']['hits']:\n",
    "                ids.append(hit['_source']['id'])\n",
    "                docs.append(hit['_source']['doc'])\n",
    "            predect.append(ids)\n",
    "            truth.append(self.rel[str(i+1)])\n",
    "            pi = precision(self.rel[str(i+1)], ids)\n",
    "            p = p + pi\n",
    "            ri = recall(self.rel[str(i+1)], ids)\n",
    "            r = r + ri\n",
    "            #print(pi,ri)\n",
    "           # f1 = f1 + (2 * ((pi * ri)/(pi + ri)))\n",
    "        return p / len(predect), r/len(predect) , f1/len(predect)\n",
    "\n",
    "nop = noprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13647058823529412, 0.32433051372235, 0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nop.searchPR(count=100, index= 'cisi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14029411764705885, 0.3392589128755711, 0.17114114617176152)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nop.searchNoSW(count=100, index= 'cisi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1423529411764706, 0.347762030237089, 0.17435062822288983)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nop.searchNoWhword(count=100,  index= 'cisi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13647058823529412, 0.32433051372235, 0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nop.searchPR(count=100, index= 'anacisi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14029411764705885, 0.3392589128755711, 0.17114114617176152)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nop.searchNoSW(count=100, index= 'anacisi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1423529411764706, 0.347762030237089, 0.17435062822288983)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nop.searchNoWhword(count=100,  index= 'anacisi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "class QueryProcessor:\n",
    "\n",
    "    def __init__(self, nlp, keep=None):\n",
    "        self.nlp = nlp\n",
    "        self.keep = keep or {'PROPN', 'NUM', 'VERB', 'NOUN', 'ADJ'}\n",
    "\n",
    "    def generate_query(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        query = ' '.join(token.text for token in doc if token.pos_ in self.keep)\n",
    "        return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'song caty perry'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp = QueryProcessor(nlp)\n",
    "qp.generate_query('what is the song for caty perry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_setting = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"my_analyzer_keyword\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"keyword\",\n",
    "                    \"filter\": [\n",
    "                        \"asciifolding\",\n",
    "                        \"lowercase\",\n",
    "                        \"synonym\"\n",
    "                    ]\n",
    "                },\n",
    "                \"my_analyzer_shingle\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"asciifolding\",\n",
    "                        \"lowercase\",\n",
    "                        \"synonym\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }, \"mappings\": {\n",
    "   \n",
    "            \"properties\": {\n",
    "                'id':{\n",
    "                    'type': \"string\"\n",
    "                },\n",
    "                \"doc\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"index_analyzer\": \"my_analyzer_keyword\",\n",
    "                    \"search_analyzer\": \"my_analyzer_shingle\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-3d9b34b92592>:1: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use the 'document' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  client.index(index='anacisi', body=doc_setting)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_index': 'anacisi', '_id': 'qiLXWYEBraLnPera6o8W', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.index(index='anacisi', body=doc_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44ee107329cf4a2b10dbf9dc2a835ad7dfaf7406a03f5d729fde60d5b9868961"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
