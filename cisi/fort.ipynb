{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "!pip install forte.elastic\n",
    "!pip install composable_source.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<forte.pipeline.Pipeline at 0x2013e09ce10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import forte\n",
    "from forte.data.readers import TerminalReader\n",
    "from fortex.nltk.nltk_processors import NLTKLemmatizer, NLTKWordTokenizer, NLTKPOSTagger, NLTKSentenceSegmenter\n",
    "from fortex.allennlp.allennlp_processors import AllenNLPProcessor\n",
    "\n",
    "nlp = forte.pipeline.Pipeline()\n",
    "\n",
    "nlp.set_reader(TerminalReader())\n",
    "\n",
    "nlp.add(NLTKSentenceSegmenter())\n",
    "nlp.add(NLTKWordTokenizer())\n",
    "nlp.add(NLTKPOSTagger())\n",
    "nlp.add(NLTKLemmatizer())\n",
    "\n",
    "allennlp_config = {\n",
    "    'processors': \"tokenize, pos, srl\",\n",
    "    'tag_formalism': \"srl\",\n",
    "    'overwrite_entries': False,\n",
    "    'allow_parallel_entries': True,\n",
    "    'srl_url': \"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\"\n",
    "}\n",
    "nlp.add(AllenNLPProcessor(), config=allennlp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Re-declared a new class named [ConstituentNode], which is probably used in import.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Mohammad\n",
      "[nltk_data]     Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mohammad Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohammad\n",
      "[nltk_data]     Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING:allennlp.common.params:error loading _jsonnet (this is expected on Windows), treating C:\\Users\\MOHAMM~1\\AppData\\Local\\Temp\\tmprg4sq43y\\config.json as plain json\n",
      "WARNING:allennlp.common.params:error loading _jsonnet (this is expected on Windows), treating C:\\Users\\MOHAMM~1\\AppData\\Local\\Temp\\tmp5rk1un1n\\config.json as plain json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens created by NLTK:\n",
      " text: What, pos: WP, lemma: What\n",
      " text: does, pos: VBZ, lemma: do\n",
      " text: nlp, pos: JJ, lemma: nlp\n",
      " text: mean, pos: VB, lemma: mean\n",
      " text: ?, pos: ., lemma: ?\n",
      "Semantic role labels created by AllenNLP:\n",
      " verb: mean, noun: What, noun_type: ARG1\n",
      " verb: mean, noun: nlp, noun_type: ARG0\n"
     ]
    }
   ],
   "source": [
    "nlp.initialize()\n",
    "from ft.onto.base_ontology import Token, Sentence, PredicateLink\n",
    "data_pack = next(nlp.process_dataset())\n",
    "for sent in data_pack.get(Sentence):\n",
    "    print(\"Tokens created by NLTK:\")\n",
    "    for token in data_pack.get(Token, sent, components=[\"fortex.nltk.nltk_processors.NLTKWordTokenizer\"]):\n",
    "        print(f\" text: {token.text}, pos: {token.pos}, lemma: {token.lemma}\")\n",
    "print(\"Semantic role labels created by AllenNLP:\")\n",
    "for pred in data_pack.get(PredicateLink, sent, components=[\"fortex.allennlp.allennlp_processors.AllenNLPProcessor\"]):\n",
    "    verb = pred.get_parent()\n",
    "    noun = pred.get_child()\n",
    "    print(f\" verb: {verb.text}, noun: {data_pack.text[noun.begin:noun.end]}, noun_type: {pred.arg_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<forte.pipeline.Pipeline at 0x2013e09ce10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from forte.data.caster import MultiPackBoxer\n",
    "boxer_config = {\"pack_name\": \"query\"}\n",
    "nlp.add(MultiPackBoxer(), config=boxer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Tuple\n",
    "from forte.data.data_pack import DataPack\n",
    "from forte.data.multi_pack import MultiPack\n",
    "from forte.processors.base import QueryProcessor\n",
    "from fortex.elastic.elastic_search_processor import ElasticSearchProcessor\n",
    "\n",
    "\n",
    "class ElasticSearchQueryCreator(QueryProcessor):\n",
    "    @classmethod\n",
    "    def default_configs(cls) -> Dict[str, Any]:\n",
    "        config = super().default_configs()\n",
    "        config.update({\n",
    "            \"size\": 1000,\n",
    "            \"field\": \"doc\",\n",
    "            \"query_pack_name\": \"query\"\n",
    "        }) \n",
    "        return config\n",
    "\n",
    "    def _process_query(self, input_pack: MultiPack) -> Tuple[DataPack, Dict[str, Any]]:\n",
    "        query_pack = input_pack.get_pack(self.configs.query_pack_name)\n",
    "        query_pack.pack_name = self.configs.query_pack_name\n",
    "        query = self._build_query_nlp(query_pack)\n",
    "        return query_pack, query\n",
    "\n",
    "    def _build_query_nlp(self, input_pack: DataPack) -> Dict[str, Any]:\n",
    "        query, arg0, arg1, verb, _, is_answer_arg0 = query_preprocess(input_pack)\n",
    "        if not arg0 or not arg1:\n",
    "            processed_query = query\n",
    "        if is_answer_arg0 is None:\n",
    "            processed_query = f'{arg0} {verb} {arg1}'.lower()\n",
    "        elif is_answer_arg0:\n",
    "            processed_query = f'{arg1} {verb}'.lower()\n",
    "        else:\n",
    "            processed_query = f'{arg0} {verb}'.lower()\n",
    "        return {\n",
    "            \"query\": {\n",
    "                \"match_phrase\": {\n",
    "                    self.configs.field: {\n",
    "                        \"query\": processed_query,\n",
    "                        \"slop\": 10\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"size\": self.configs.size\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<forte.pipeline.Pipeline at 0x2013e09ce10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_creator_config = {\"size\": 10}\n",
    "nlp.add(ElasticSearchQueryCreator(), query_creator_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Re-declared a new class named [ConstituentNode], which is probably used in import.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Mohammad\n",
      "[nltk_data]     Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mohammad Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohammad\n",
      "[nltk_data]     Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING:allennlp.common.params:error loading _jsonnet (this is expected on Windows), treating C:\\Users\\MOHAMM~1\\AppData\\Local\\Temp\\tmpe4n4rqrv\\config.json as plain json\n",
      "WARNING:allennlp.common.params:error loading _jsonnet (this is expected on Windows), treating C:\\Users\\MOHAMM~1\\AppData\\Local\\Temp\\tmp6ecms6xo\\config.json as plain json\n",
      "ERROR:forte.pipeline:value of the packs should be DataPack, but got forte.data.multi_pack.MultiPack\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\pipeline.py\", line 1089, in _process_with_component\n",
      "    raw_job.alter_pack(component.cast(pack))\n",
      "  File \"c:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\data\\caster.py\", line 73, in cast\n",
      "    p.add_pack_(pack, self.configs.pack_name)\n",
      "  File \"c:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\data\\multi_pack.py\", line 460, in add_pack_\n",
      "    f\"value of the packs should be DataPack, but \"\n",
      "ValueError: value of the packs should be DataPack, but got forte.data.multi_pack.MultiPack\n"
     ]
    },
    {
     "ename": "ProcessExecutionException",
     "evalue": "Exception occurred when running forte.data.caster.MultiPackBoxer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\pipeline.py\u001b[0m in \u001b[0;36m_process_with_component\u001b[1;34m(self, selector, component, raw_job)\u001b[0m\n\u001b[0;32m   1088\u001b[0m                     \u001b[1;31m# Replacing the job pack with the casted version.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m                     \u001b[0mraw_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malter_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseBatchProcessor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\data\\caster.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(self, pack)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultiPack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpack_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pack_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\data\\multi_pack.py\u001b[0m in \u001b[0;36madd_pack_\u001b[1;34m(self, pack, ref_name)\u001b[0m\n\u001b[0;32m    459\u001b[0m             raise ValueError(\n\u001b[1;32m--> 460\u001b[1;33m                 \u001b[1;34mf\"value of the packs should be DataPack, but \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m                 \u001b[1;34mf\"got {type(pack)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: value of the packs should be DataPack, but got forte.data.multi_pack.MultiPack",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProcessExecutionException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e9bf8dd96e8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mforte\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0montology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQuery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"query\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\pipeline.py\u001b[0m in \u001b[0;36m_process_packs\u001b[1;34m(self, data_iter)\u001b[0m\n\u001b[0;32m   1260\u001b[0m                     \u001b[0mstart_time\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1262\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_with_component\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_job\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m                 \u001b[1;31m# Stop timer and add to time profiler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\pipeline.py\u001b[0m in \u001b[0;36m_process_with_component\u001b[1;34m(self, selector, component, raw_job)\u001b[0m\n\u001b[0;32m   1109\u001b[0m                 raise ProcessExecutionException(\n\u001b[0;32m   1110\u001b[0m                     \u001b[1;34mf\"Exception occurred when running \"\u001b[0m \u001b[1;34mf\"{component.name}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m                 ) from e\n\u001b[0m\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m     def _process_packs(\n",
      "\u001b[1;31mProcessExecutionException\u001b[0m: Exception occurred when running forte.data.caster.MultiPackBoxer"
     ]
    }
   ],
   "source": [
    "from forte.data.ontology.top import Query\n",
    "nlp.initialize()\n",
    "print(next(nlp.process_dataset()).get_pack(\"query\").get_single(Query).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<forte.pipeline.Pipeline at 0x2013e09ce10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fortex.elastic import ElasticSearchProcessor\n",
    "elastic_search_config = {\n",
    "    \"query_pack_name\": \"query\",\n",
    "    \"index_config\":{\n",
    "        \"index_name\": \"cisi\",\n",
    "        \"hosts\": \"localhost:9200\",\n",
    "        \"algorithm\": \"bm25\",\n",
    "    },\n",
    "    \"field\": \"content\",\n",
    "    \"response_pack_name_prefix\": \"passage\",\n",
    "    \"indexed_text_only\": False\n",
    "}\n",
    "nlp.add(ElasticSearchProcessor(), elastic_search_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Re-declared a new class named [ConstituentNode], which is probably used in import.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Mohammad\n",
      "[nltk_data]     Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mohammad Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohammad\n",
      "[nltk_data]     Dareer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING:allennlp.common.params:error loading _jsonnet (this is expected on Windows), treating C:\\Users\\MOHAMM~1\\AppData\\Local\\Temp\\tmp6509ijxk\\config.json as plain json\n",
      "WARNING:allennlp.common.params:error loading _jsonnet (this is expected on Windows), treating C:\\Users\\MOHAMM~1\\AppData\\Local\\Temp\\tmptl0dwp64\\config.json as plain json\n",
      "ERROR:forte.pipeline:value of the packs should be DataPack, but got forte.data.multi_pack.MultiPack\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\pipeline.py\", line 1089, in _process_with_component\n",
      "    raw_job.alter_pack(component.cast(pack))\n",
      "  File \"c:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\data\\caster.py\", line 73, in cast\n",
      "    p.add_pack_(pack, self.configs.pack_name)\n",
      "  File \"c:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\data\\multi_pack.py\", line 460, in add_pack_\n",
      "    f\"value of the packs should be DataPack, but \"\n",
      "ValueError: value of the packs should be DataPack, but got forte.data.multi_pack.MultiPack\n"
     ]
    },
    {
     "ename": "ProcessExecutionException",
     "evalue": "Exception occurred when running forte.data.caster.MultiPackBoxer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\pipeline.py\u001b[0m in \u001b[0;36m_process_with_component\u001b[1;34m(self, selector, component, raw_job)\u001b[0m\n\u001b[0;32m   1088\u001b[0m                     \u001b[1;31m# Replacing the job pack with the casted version.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m                     \u001b[0mraw_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malter_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseBatchProcessor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\data\\caster.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(self, pack)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultiPack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpack_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pack_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\data\\multi_pack.py\u001b[0m in \u001b[0;36madd_pack_\u001b[1;34m(self, pack, ref_name)\u001b[0m\n\u001b[0;32m    459\u001b[0m             raise ValueError(\n\u001b[1;32m--> 460\u001b[1;33m                 \u001b[1;34mf\"value of the packs should be DataPack, but \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m                 \u001b[1;34mf\"got {type(pack)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: value of the packs should be DataPack, but got forte.data.multi_pack.MultiPack",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProcessExecutionException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4d5afd678985>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mm_pack\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Question: {m_pack.get_pack('query').text}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Results\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpack\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mm_pack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\pipeline.py\u001b[0m in \u001b[0;36m_process_packs\u001b[1;34m(self, data_iter)\u001b[0m\n\u001b[0;32m   1260\u001b[0m                     \u001b[0mstart_time\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1262\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_with_component\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_job\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m                 \u001b[1;31m# Stop timer and add to time profiler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mohammad Dareer\\anaconda3\\envs\\forte_qa\\lib\\site-packages\\forte\\pipeline.py\u001b[0m in \u001b[0;36m_process_with_component\u001b[1;34m(self, selector, component, raw_job)\u001b[0m\n\u001b[0;32m   1109\u001b[0m                 raise ProcessExecutionException(\n\u001b[0;32m   1110\u001b[0m                     \u001b[1;34mf\"Exception occurred when running \"\u001b[0m \u001b[1;34mf\"{component.name}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m                 ) from e\n\u001b[0m\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m     def _process_packs(\n",
      "\u001b[1;31mProcessExecutionException\u001b[0m: Exception occurred when running forte.data.caster.MultiPackBoxer"
     ]
    }
   ],
   "source": [
    "nlp.initialize()\n",
    "for m_pack in nlp.process_dataset():\n",
    "    print(f\"Question: {m_pack.get_pack('query').text}\")\n",
    "    print(\"Results\")\n",
    "    for pack in m_pack.packs:\n",
    "        if pack.pack_name != \"query\":\n",
    "            print(f\" {pack.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, DefaultDict\n",
    "from ft.onto.base_ontology import Token, Sentence, PredicateLink, Annotation\n",
    "from forte.data.data_pack import DataPack\n",
    "\n",
    "\n",
    "def query_preprocess(input_pack: DataPack):\n",
    "    \"\"\"\n",
    "    Extract nouns and verb from user input query.\n",
    "    :param input_pack:\n",
    "    :return:sentence: query text\n",
    "        arg0: subject in query\n",
    "        arg1: object in query\n",
    "        predicate: verb in query\n",
    "        verb_lemma: verb lemma\n",
    "        is_answer_arg0: should subject(arg0) or object(arg1) be returned\n",
    "        as answer\n",
    "    \"\"\"\n",
    "    sentence = input_pack.get_single(Sentence)\n",
    "\n",
    "    relations: DefaultDict[str, Dict[str, Dict[str, str]]] = defaultdict(dict)\n",
    "    text_mention_mapping = {}\n",
    "\n",
    "    # get all srl relations\n",
    "    for link in input_pack.get(PredicateLink, sentence):\n",
    "        verb = link.get_parent()\n",
    "        verb_text = verb.text\n",
    "        argument = link.get_child()\n",
    "        argument_text = argument.text\n",
    "\n",
    "        text_mention_mapping[verb_text] = verb\n",
    "        text_mention_mapping[argument_text] = argument\n",
    "        relations[verb_text][link.arg_type] = argument_text\n",
    "\n",
    "    arg0, arg1, predicate = \"\", \"\", \"\"\n",
    "    for verb_text, entity in relations.items():\n",
    "        arg0, arg1, predicate = collect_mentions(\n",
    "            text_mention_mapping, entity, verb_text\n",
    "        )\n",
    "        if arg0 == \"\" and arg1 == \"\":\n",
    "            continue\n",
    "\n",
    "    assert (\n",
    "        isinstance(arg0, Annotation)\n",
    "        and isinstance(arg1, Annotation)\n",
    "        and isinstance(predicate, Annotation)\n",
    "    ), (\n",
    "        \"AllenNLP SRL cannot extract the two arguments or the \"\n",
    "        \"predicate in your query, please check our examples \"\n",
    "        \"or rephrase your question\"\n",
    "    )\n",
    "\n",
    "    verb_lemma, is_answer_arg0 = None, None\n",
    "\n",
    "    # check pos tag and lemma for tokens\n",
    "    for token in input_pack.get(\n",
    "        entry_type=Token,\n",
    "        range_annotation=sentence,\n",
    "        components=[\"fortex.nltk.nltk_processors.NLTKWordTokenizer\"],\n",
    "    ):\n",
    "        # find WH words\n",
    "        if token.pos in {\"WP\", \"WP$\", \"WRB\", \"WDT\"}:\n",
    "            if arg0.begin <= token.begin and arg0.end >= token.end:\n",
    "                is_answer_arg0 = True\n",
    "            elif arg1.begin <= token.begin and arg1.end >= token.end:\n",
    "                is_answer_arg0 = False\n",
    "\n",
    "        # find verb lemma\n",
    "        if token.text == predicate.text:\n",
    "            verb_lemma = token.lemma\n",
    "\n",
    "    return (\n",
    "        sentence,\n",
    "        arg0.text if arg0 else \"\",\n",
    "        arg1.text if arg1 else \"\",\n",
    "        predicate.text,\n",
    "        verb_lemma,\n",
    "        is_answer_arg0,\n",
    "    )\n",
    "\n",
    "\n",
    "def collect_mentions(text_mention_mapping, relation, verb_text):\n",
    "    \"\"\"\n",
    "    Get arg0,arg1 and predicate entity mention\n",
    "    :param text_mention_mapping:\n",
    "    :param relation:\n",
    "    :param verb_text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    arg0_text, arg1_text = get_arg_text(relation)\n",
    "\n",
    "    if arg0_text == \"\" or arg1_text == \"\":\n",
    "        return \"\", \"\", \"\"\n",
    "\n",
    "    arg0 = text_mention_mapping[arg0_text]\n",
    "    arg1 = text_mention_mapping[arg1_text]\n",
    "    predicate = text_mention_mapping[verb_text]\n",
    "\n",
    "    return arg0, arg1, predicate\n",
    "\n",
    "\n",
    "def get_arg_text(relation):\n",
    "    \"\"\"\n",
    "    find arg0 and arg1 text in all relations. we considered 3 annotation\n",
    "    for comprehensive subject and object extraction\n",
    "    As AllenNLP uses PropBank Annotation, each verb sense has numbered\n",
    "    arguments e.g., ARG-0, ARG-1, etc.\n",
    "    ARG-0 is usually PROTO-AGENT\n",
    "    ARG-1 is usually PROTO-PATIENT\n",
    "    ARG-2 is usually benefactive, instrument, attribute\n",
    "    :param relation:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    arg0_text, arg1_text = \"\", \"\"\n",
    "    if \"ARG0\" in relation and \"ARG1\" in relation:\n",
    "        arg0_text = relation[\"ARG0\"]\n",
    "        arg1_text = relation[\"ARG1\"]\n",
    "\n",
    "    elif \"ARG1\" in relation and \"ARG2\" in relation:\n",
    "        arg0_text = relation[\"ARG1\"]\n",
    "        arg1_text = relation[\"ARG2\"]\n",
    "\n",
    "    return arg0_text, arg1_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8003d64d6ab2671bb91cb89ee1f606d5c1ea1755ff079991a16b43ffa3f87471"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('forte_qa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
