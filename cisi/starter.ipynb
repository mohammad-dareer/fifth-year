{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from elasticsearch import Elasticsearch\n",
    "import sklearn.metrics as sl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import helpers\n",
    "client = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "     basic_auth=(\"elastic\", \"elastic\")\n",
    "\n",
    ")\n",
    "resp = client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/data/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        with open(os.path.join(dirname, filename)) as f:\n",
    "            line_count = 0\n",
    "            id_set = set()\n",
    "            for l in f.readlines():\n",
    "                line_count += 1\n",
    "                if filename == \"CISI.REL\":\n",
    "                    id_set.add(l.lstrip(\" \").split(\" \")[0])\n",
    "                elif l.startswith(\".I \"):\n",
    "                    id_set.add(l.split(\" \")[1].strip())\n",
    "            print(f\"{filename} : {len(id_set)} items, over {line_count} lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['60', '85', '114', '123', '126', '131', '133', '136', '138', '140', '346', '359', '363', '372', '412', '445', '454', '461', '463', '469', '532', '537', '540', '553', '554', '555', '585', '590', '599', '640', '660', '664', '803', '901', '909', '911', '1027', '1053', '1169', '1179', '1181', '1190', '1191', '1326']\n"
     ]
    }
   ],
   "source": [
    "temp_id = 0\n",
    "temp_doc_id = []\n",
    "rel = {}\n",
    "with open('data\\CISI.REL') as f:\n",
    "    lines = \"\"\n",
    "    for l in f.readlines():\n",
    "        qid = l.strip().split()[0]\n",
    "        if(qid == temp_id):\n",
    "            temp_doc_id.append(l.strip().split()[1])\n",
    "        else:\n",
    "            rel[temp_id] = temp_doc_id\n",
    "            temp_id = qid\n",
    "            temp_doc_id = []\n",
    "            temp_doc_id.append(l.strip().split()[1])\n",
    "rel.pop(0) \n",
    "print(rel['3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7475\n"
     ]
    }
   ],
   "source": [
    "with open('data\\CISI.ALL') as f:\n",
    "    lines = \"\"\n",
    "    for l in f.readlines():\n",
    "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "    \n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_list =[]\n",
    "qry_id = \"\"\n",
    "qry_text = \"\"\n",
    "with open('data\\CISI.QRY') as f:\n",
    "    qry = \"\"\n",
    "    for l in f.readlines():\n",
    "        qry += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "    qry = qry.lstrip(\"\\n\").split(\"\\n\")\n",
    "for l in qry:\n",
    "    if l.startswith(\".I\"):\n",
    "        qry_list.append({'id':qry_id, 'doc':qry_text.lstrip(\" \")})\n",
    "        qry_id = l.split(\" \")[1].strip()\n",
    "        qry_text = \"\"\n",
    "    \n",
    "    else:\n",
    "        qry_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.\n",
    "\n",
    "\n",
    "qry_list.remove({'id':'' ,'doc':''})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents = 1460.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_set = {}\n",
    "doc_list = []\n",
    "doc_id = \"\"\n",
    "doc_text = \"\"\n",
    "ind = 1\n",
    "doc_df = pd.DataFrame(columns=['id','doc'])\n",
    "for l in lines:\n",
    "    if l.startswith(\".I\"):\n",
    "        doc_id = l.split(\" \")[1].strip()\n",
    "    elif l.startswith(\".X\"):\n",
    "        doc_df.loc[ind] = [doc_id, doc_text.lstrip(\" \")]\n",
    "        ind = ind + 1\n",
    "       # doc_df.append({'id':doc_id,'doc':doc_text.lstrip(\" \")}, ignore_index=True)\n",
    "       # doc_df['doc'] = doc_text.lstrip(\" \")\n",
    "        doc_set['doc'] = doc_text.lstrip(\" \")\n",
    "        doc_set['id'] = doc_id\n",
    "        doc_list.append({'id':doc_id, 'doc':doc_text.lstrip(\" \")})\n",
    "        doc_set['doc'] = \"\"\n",
    "        doc_set['id'] = \"\"\n",
    "        doc_id = \"\"\n",
    "        doc_text = \"\"\n",
    "    \n",
    "    else:\n",
    "        doc_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.\n",
    "\n",
    "# Print something to see the dictionary structure, etc.\n",
    "print(f\"Number of documents = {len(doc_list)}\" + \".\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for rec in doc_list:\n",
    "    i = i + 1\n",
    "    client.index(index=\"anacisi\", id = i, document=rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = client.search(index=\"cisi\", query={\"match_all\": {}})\n",
    "print(\"Got %d Hits:\" % all['hits']['total']['value'])\n",
    "for hit in all['hits']['hits']:\n",
    "    print(\"%(doc)s : %(id)s\" % hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is information science?  Give definitions where possible. \n",
      "Got 1311 Hits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "truth =[]\n",
    "predect = []\n",
    "for i in range(2,3):\n",
    "    print(qry_list[i]['doc'] )\n",
    "    query = {\n",
    "                \"match\": {\n",
    "                    \"doc\": qry_list[i]['doc'] \n",
    "                }\n",
    "            }\n",
    "    ids =[]\n",
    "    docs =[]\n",
    "    resp = client.search(index=\"cisi\", query=query, size=100)\n",
    "    print(\"Got %d Hits\" % resp['hits']['total']['value'])\n",
    "    for hit in resp['hits']['hits']:\n",
    "        # print(\"%(id)s \" % hit[\"_source\"])\n",
    "        ids.append(hit['_source']['id'])\n",
    "        docs.append(hit['_source']['doc'])\n",
    "    predect.append(ids)\n",
    "  \n",
    "    truth.append(rel[str(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8bb7d309f4b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sl' is not defined"
     ]
    }
   ],
   "source": [
    "sl.precision_score(truth,predect,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true:list, predect:list):\n",
    "    f = 0\n",
    "    for t in true :\n",
    "        for p in predect:\n",
    "            if p == t:\n",
    "                f = f + 1\n",
    "    return f /len(predect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(true:list, predect:list):\n",
    "    f = 0\n",
    "    for t in true :\n",
    "        for p in predect:\n",
    "            if p == t:\n",
    "                f = f + 1\n",
    "    return f /len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14\n",
      "0.3181818181818182\n"
     ]
    }
   ],
   "source": [
    "print(precision(truth[0],predect[0]))\n",
    "print(recall(truth[0],predect[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryProcessor:\n",
    "\n",
    "    def __init__(self, nlp, keep=None):\n",
    "        self.nlp = nlp\n",
    "        self.keep = keep or {'PROPN', 'NUM', 'VERB', 'NOUN', 'ADJ'}\n",
    "\n",
    "    def generate_query(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        query = ' '.join(token.text for token in doc if token.pos_ in self.keep)\n",
    "        return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-346669fe367f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mqp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQueryProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'nlp'"
     ]
    }
   ],
   "source": [
    "qp = QueryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44ee107329cf4a2b10dbf9dc2a835ad7dfaf7406a03f5d729fde60d5b9868961"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
